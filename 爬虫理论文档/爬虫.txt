爬虫:模拟客户端发送网络请求,接收请求对应的响应,一种按照一定的规则,自动地抓取互联网信息的程序.

分类
通用爬虫:搜索引擎
聚焦爬虫:针对特定网站的爬虫,定向的获取某方面数据的爬虫
	累积式爬虫:从开始到结束,不断爬取,过程中会进行去重操作
	增量式爬虫:已下载网页采取增量式更新和只爬行新产生的或者已经发生变化网页的爬虫
	Deep web爬虫:不能通过静态链接获取的,隐藏在搜索表单后的,只有用户提交一些关键词才能获得的web页面

 
HTTP常见请求头:
1.Host(主机和端口号)
2.Connection(链接类型)
3.Upgrade-Lnsecure-Requests(升级为HTTPS请求)
4.User-Agent(用户代理)
	服务器能够识别客户使用操作系统及版本,CPU类型,浏览器及版本,浏览器渲染引擎,浏览器语言,浏览器插件等
5.Accept(传输文件类型)
6.Referer(页面跳转处)
7.Accept-Encoding(文件编解码格式)
8.Cookie(Cookie)
9.x-requested-with:XMLHTTPRequest 是Ajax异步请求
 

爬虫工作原理
1.目标的url

2.代码发送请求

3.解析数据

4.入库

ROBOTS协议:机器人协议,爬虫不遵守


HTTP和HTTPS
端口:80;443
安全性:不安全;SSL(安全套接字层)ca证书


GET
・用来请求资源
・在url中传输实体数据
・传输的数据量小

#状态码
code = response.status_code

#请求头
request_headers = response.request.headers


#将cookiejar转换成字典
login_response = session.post(login_url,data=login_data,headers=headers)

#使用requests.session处理cookie
session = requests.session()
response = session.get(url,headers=headers)



#通过cookieJar将cookie转换成字典形式
import requests
url = "http://www.baidu.com"
#发送请求，获取resposne
response = requests.get(url)
print(type(response.cookies))

#使用方法从cookiejar中提取数据
cookies = requests.utils.dict_from_cookiejar(response.cookies)
print(cookies)



#忽略ca证书认证
response = requests.get(url,headers=headers, verify=False)
cookies = login_response.request._cookies
cookies_dict = requests.utils.dict_from_cookiejar(cookies)


#超时参数的使用
response = requests.get(url,timeout=3)--->通过timeout参数,能够保证在3秒内返回响应,否则会报错

#retrying模块的使用
retrying 模块的使用
使用retrying模块提供的retry模块
通过装饰器的方式使用，让被装饰的函数反复执行
retry中可以传入参数stop_max_attempt_number,让函数报错后继续重新执行，达到最大执行次数的上限，如果每次都报错，整个函数报错，如果中间有一个成功，程序继续往后执行




import requests
from retrying import retry

headers = {}

#最大重试3次，3次全部报错，才会报错
@retry(stop_max_attempt_number=3) 
def _parse_url(url)
    #超时的时候回报错并重试
    response = requests.get(url, headers=headers, timeout=3) 
    #状态码不是200，也会报错并重试
    assert response.status_code == 200
    return response

def parse_url(url)
    try: #进行异常捕获
        response = _parse_url(url)
    except Exception as e:
        print(e)
        #报错返回None
        response = None
    return response




#proxy代理ip
代理IP 免费IP{"协议":"IP:port"}
{"协议":"username:pwd@IP:port"}
proxy = {
	"http":"61.135.217.7:80"
}
response = requests.get(url,headers=headers,proxies=proxy


#批量IP代理
proxy_list = [
	{"协议":"IP:PORT"},
	{"协议":"IP:PORT"},



结构化数据与非结构化数据
结构化数据:现有结构后有数据-->json;xml
非结构数据:小说,文本,电话,邮箱;html
xml:数据传输格式
HTML:展示数据